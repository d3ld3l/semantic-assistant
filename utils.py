# utils.py

import pandas as pd
import requests
import re
from io import BytesIO
from sentence_transformers import SentenceTransformer, util

# Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð¸ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# Ð¡ÑÑ‹Ð»ÐºÐ¸ Ð½Ð° Excel-Ñ„Ð°Ð¹Ð»Ñ‹ Ð² GitHub
GITHUB_CSV_URLS = [
    "https://raw.githubusercontent.com/d3ld3l/semantic-assistant/main/data1.xlsx",
    "https://raw.githubusercontent.com/d3ld3l/semantic-assistant/main/data2.xlsx",
    "https://raw.githubusercontent.com/d3ld3l/semantic-assistant/main/data3.xlsx"
]

# ðŸ” Ð¡Ð»Ð¾Ð²Ð°Ñ€ÑŒ ÑÐ¸Ð½Ð¾Ð½Ð¸Ð¼Ð¾Ð² (Ð¼Ð¾Ð¶Ð½Ð¾ Ñ€Ð°ÑÑˆÐ¸Ñ€ÑÑ‚ÑŒ)
SYNONYMS = {
    "ÑÐ¸Ð¼ÐºÐ°": "ÑÐ¸Ð¼ÐºÐ°Ñ€Ñ‚Ð°",
    "ÐºÑ€ÐµÐ´Ð¸Ñ‚ÐºÐ°": "ÐºÑ€ÐµÐ´Ð¸Ñ‚Ð½Ð°Ñ ÐºÐ°Ñ€Ñ‚Ð°",
    "Ð½Ð¾ÑƒÑ‚": "Ð½Ð¾ÑƒÑ‚Ð±ÑƒÐº",
    "ÐºÐ¾Ð¼Ð¿": "ÐºÐ¾Ð¼Ð¿ÑŒÑŽÑ‚ÐµÑ€",
    "Ñ‚ÐµÐ»Ð¸Ðº": "Ñ‚ÐµÐ»ÐµÐ²Ð¸Ð·Ð¾Ñ€",
    "Ð°Ð¹Ñ„Ð¾Ð½": "iphone",
    "Ð°Ð½Ð´Ñ€Ð¾Ð¸Ð´": "android",
    "Ð²Ð¸Ð´Ð¾Ñ": "Ð²Ð¸Ð´ÐµÐ¾",
    "Ñ„Ð¾Ñ‚ÐºÐ°": "Ñ„Ð¾Ñ‚Ð¾Ð³Ñ€Ð°Ñ„Ð¸Ñ"
}

# ðŸ”§ ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð° + Ð·Ð°Ð¼ÐµÐ½Ð° ÑÐ¸Ð½Ð¾Ð½Ð¸Ð¼Ð¾Ð²
def normalize_synonyms(text):
    for word, replacement in SYNONYMS.items():
        text = re.sub(rf"\b{re.escape(word)}\b", replacement, text)
    return text

def preprocess(text):
    text = str(text).lower().strip()
    text = normalize_synonyms(text)
    text = re.sub(r"\s+", " ", text)
    return text

# ðŸ“¥ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¾Ð´Ð½Ð¾Ð³Ð¾ Excel-Ñ„Ð°Ð¹Ð»Ð°
def load_excel(url):
    response = requests.get(url)
    if response.status_code != 200:
        raise ValueError(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ {url}")
    df = pd.read_excel(BytesIO(response.content))
    
    topic_cols = [col for col in df.columns if col.lower().startswith("topics")]
    if not topic_cols:
        raise KeyError("ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ topics")

    df = df[['phrase'] + topic_cols]
    df['topics'] = df[topic_cols].fillna('').agg(lambda x: [t for t in x.tolist() if t], axis=1)
    df['phrase_proc'] = df['phrase'].apply(preprocess)
    return df[['phrase', 'phrase_proc', 'topics']]

# ðŸ”„ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð²ÑÐµÑ… Ñ„Ð°Ð¹Ð»Ð¾Ð²
def load_all_excels():
    dfs = []
    for url in GITHUB_CSV_URLS:
        try:
            df = load_excel(url)
            dfs.append(df)
        except Exception as e:
            print(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ñ {url}: {e}")
    if not dfs:
        raise ValueError("âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð½Ð¸ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ñ„Ð°Ð¹Ð»Ð°")
    return pd.concat(dfs, ignore_index=True)

# ðŸ” Ð¡ÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ð¸ÑÐº
def semantic_search(query, df, top_k=5, threshold=0.5):
    query_proc = preprocess(query)
    query_emb = model.encode(query_proc, convert_to_tensor=True)
    phrase_embs = model.encode(df['phrase_proc'].tolist(), convert_to_tensor=True)

    sims = util.pytorch_cos_sim(query_emb, phrase_embs)[0]
    results = []

    for idx, score in enumerate(sims):
        score = float(score)
        if score >= threshold:
            phrase = df.iloc[idx]['phrase']
            topics = df.iloc[idx]['topics']
            results.append((score, phrase, topics))

    results.sort(key=lambda x: x[0], reverse=True)
    return results[:top_k]
